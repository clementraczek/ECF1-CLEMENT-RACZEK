# ECF Data Engineering : Pipeline Multi-Sources & Analytics (Medallion)

## ğŸ“ Description
Ce projet implÃ©mente un pipeline de donnÃ©es ETL (Extract, Transform, Load) complet pour collecter, transformer et analyser des donnÃ©es provenant de quatre sources distinctes. L'objectif est de dÃ©montrer la capacitÃ© Ã  orchestrer des flux de donnÃ©es complexes, Ã  gÃ©rer une infrastructure hybride (S3/SQL) et Ã  assurer la conformitÃ© et la qualitÃ© des donnÃ©es.

**Sources de donnÃ©es :**
- **Books to Scrape** : Catalogue de livres (Prix, notes, thÃ©matiques).
- **Quotes to Scrape** : Citations et mÃ©tadonnÃ©es auteurs.
- **E-commerce Site** : DonnÃ©es techniques et tarifs informatiques (Laptops).
- **Partenaires** : DonnÃ©es de gÃ©olocalisation des librairies (API GÃ©o).

## Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SOURCES DU WEB              â”‚
â”‚ (Books, Quotes, E-commerce, Librairies)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Scraping (Scrapy / Python)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PIPELINE                  â”‚
â”‚      Extract â†’ Transform â†’ Load          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚   MinIO   â”‚            â”‚ PostgreSQLâ”‚
    â”‚ (Bronze/Silver)        â”‚   (Gold)  â”‚
    â”‚           â”‚            â”‚           â”‚
    â”‚ Fichiers Bruts         â”‚ Tables de Faits
    â”‚ DonnÃ©es Cleaned        â”‚ Rapports SQL

```
## Justification de l'architecture hybride

L'architecture repose sur le schÃ©ma **Medallion**, garantissant une sÃ©paration stricte des responsabilitÃ©s et une traÃ§abilitÃ© totale des donnÃ©es.



| Couche | Technologie | RÃ´le |
| :--- | :--- | :--- |
| **Bronze** | **MinIO (S3)** | Stockage des fichiers bruts (JSON/XLSX) tels qu'extraits des scrapers. |
| **Silver** | **MinIO (S3)** | DonnÃ©es nettoyÃ©es, dÃ©doublonnÃ©es et converties au format CSV. |
| **Gold** | **PostgreSQL** | DonnÃ©es enrichies, anonymisÃ©es et structurÃ©es pour le reporting final. |

---

## DÃ©marrage rapide

## PrÃ©requis

- Python 3.10+
- Docker et Docker Compose
- Git

## Installation


```bash
# 1. Cloner le projet
git clone <url-du-repo>
cd ECF_1_Clement_Raczek

# 2. CrÃ©er l'environnement virtuel
python -m venv venv

# 3. Activer l'environnement virtuel
# Sur Windows :
.\venv\Scripts\activate
# Sur Linux/Mac :
source venv/bin/activate

# 4. Installer les dÃ©pendances
pip install --upgrade pip
pip install -r requirements.txt

# 5. Lancer l'infrastructure (MinIO & Postgres)
docker-compose up -d

## VÃ©rification de l'infrastructure

- **MinIO Console** : http://localhost:9001
  - Login : `minioadmin`
  - Password : `minioadmin123`

- **PostGreSQL** : http://localhost:5433
  - Base de donnÃ©es : `analytics`
  - Login : `dataeng`
  - Password : `dataeng123`

## Utilisation

# ExÃ©cuter le pipeline

**Lancement complet (Reset + Ingest + Clean + Gold)**
python -m src.pipeline --all

**Lancement complet avec rÃ©sumÃ© statistique final**
python -m src.pipeline --all --analytics

**Uniquement la phase d'extraction (Bronze)**
python -m src.pipeline --ingest

**Uniquement la phase de transformation (Silver)**
python -m src.pipeline --clean

**Uniquement l'injection en base de donnÃ©es et reporting (Gold)**
python -m src.pipeline --gold


### Options disponibles

--all	ExÃ©cute la totalitÃ© du pipeline ETL
--ingest	Lance uniquement les scrapers (Books, Quotes, Commerce)
--clean	Lance les scripts de nettoyage Pandas
--gold	Lance l'injection PostgreSQL et gÃ©nÃ¨re le rapport Excel



## Structure du projet

```
ECF_1_Clement_Raczek/
â”œâ”€â”€ config/             # ParamÃ¨tres MinIO, DB et API
â”œâ”€â”€ sql/                # Scripts et rapports Excel gÃ©nÃ©rÃ©s
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/      # Scrapers (Bronze)
â”‚   â”œâ”€â”€ processing/     # Nettoyage et Loading (Silver/Gold)
â”‚   â”œâ”€â”€ storage/        # Clients MinIO et Scripts Reset
â”‚   â”œâ”€â”€ analytics/      # Vues SQL et tests de qualitÃ©
â”‚   â””â”€â”€ pipeline.py     # Orchestrateur principal
â”œâ”€â”€ docker-compose.yml  # Infrastructure
â””â”€â”€ requirements.txt    # DÃ©pendances (incluant xlsxwriter)
```






### Variables d'environnement (.env)

```env
# ================================
# Configuration MinIO (Data Lake)
# ================================
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin123
MINIO_SECURE=false

MINIO_BUCKET_BRONZE=bronze
MINIO_BUCKET_SILVER=silver
MINIO_BUCKET_GOLD=gold


# ================================
# Configuration PostgreSQL
# ================================
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=analytics
POSTGRES_USER=dataeng
POSTGRES_PASSWORD=dataeng123


# ================================
# Configuration Pipeline
# ================================
PIPELINE_ENV=dev
SCRAPER_DELAY_SECONDS=1
LOG_LEVEL=INFO

```

### Modification de la configuration

Ã‰ditez `config/settings.py` pour modifier :
- URLs et credentials
- DÃ©lai entre requÃªtes
- Nombre max de pages
- Noms des buckets

## Analytics disponibles

Le pipeline gÃ©nÃ¨re automatiquement :
- Un rapport excel reprenant une vue globale des tables mais limitÃ©s Ã  100 lignes par requÃªte
- Un raport excel rÃ©pondant aux questions de l'ECF



## Ressources

## Ressources

## Scraping & Extraction
* [Scrapy Documentation](https://docs.scrapy.org/en/latest/) : Framework principal utilisÃ© pour l'orchestration des spiders Books et Quotes.
* [webscraper.io Test Sites](https://webscraper.io/test-sites) : Plateforme cible pour l'apprentissage du scraping e-commerce.
* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) : BibliothÃ¨que utilisÃ©e pour le parsing chirurgical des donnÃ©es e-commerce.

## Stockage & Infrastructure
* [MinIO Python SDK](https://min.io/docs/minio/linux/developers/python/minio-py.html) : Gestion du stockage objet S3 pour les couches Bronze et Silver.
* [PostgreSQL Documentation](https://www.postgresql.org/docs/) : Moteur de base de donnÃ©es relationnelle pour la couche Gold.
* [SQLAlchemy & Pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) : Outils de Mapping Objet-Relationnel (ORM) et d'injection massive de donnÃ©es.

## Architecture & QualitÃ©
* [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) : Concept de structuration des donnÃ©es par niveaux de qualitÃ© (Bronze, Silver, Gold).
* [Data Quality in ETL](https://www.metabase.com/learn/data-stack/data-quality) : Principes de validation SQL implÃ©mentÃ©s dans `sql_test.py`.
